{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, classification_report, precision_score, recall_score, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'data_with_distances.xlsx'\n",
    "updated_data = pd.read_excel(file_path)\n",
    "\n",
    "# Prepare the data for machine learning model\n",
    "features = updated_data[['Semester', 'InfoVeranstaltung', 'Note HZB', 'Note Bachelor', 'ECTS Bachelor', 'Distance_to_71034']]\n",
    "features = features.copy()\n",
    "features.loc[:, 'InfoVeranstaltung'] = features['InfoVeranstaltung'].map({'ja': 1, 'nein': 0})\n",
    "target = updated_data['Immatrikulation'].map({'Ja': 1, 'Nein': 0})\n",
    "features_filled = features.apply(lambda x: pd.to_numeric(x, errors='coerce')).fillna(features.mean())\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_filled, target, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Function to plot learning curves\n",
    "def plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None,\n",
    "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    if axes is None:\n",
    "        _, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "    axes[0].set_title(title)\n",
    "    if ylim is not None:\n",
    "        axes[0].set_ylim(*ylim)\n",
    "    axes[0].set_xlabel(\"Training examples\")\n",
    "    axes[0].set_ylabel(\"Score\")\n",
    "\n",
    "    train_sizes, train_scores, test_scores, fit_times, _ = \\\n",
    "        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n",
    "                       train_sizes=train_sizes,\n",
    "                       return_times=True)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    fit_times_mean = np.mean(fit_times, axis=1)\n",
    "    fit_times_std = np.std(fit_times, axis=1)\n",
    "\n",
    "    # Plot learning curve\n",
    "    axes[0].grid()\n",
    "    axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                         train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                         color=\"r\")\n",
    "    axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                         test_scores_mean + test_scores_std, alpha=0.1,\n",
    "                         color=\"g\")\n",
    "    axes[0].plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "                 label=\"Training score\")\n",
    "    axes[0].plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "                 label=\"Cross-validation score\")\n",
    "    axes[0].legend(loc=\"best\")\n",
    "\n",
    "    # Plot n_samples vs fit_times\n",
    "    axes[1].grid()\n",
    "    axes[1].plot(train_sizes, fit_times_mean, 'o-')\n",
    "    axes[1].fill_between(train_sizes, fit_times_mean - fit_times_std,\n",
    "                         fit_times_mean + fit_times_std, alpha=0.1)\n",
    "    axes[1].set_xlabel(\"Training examples\")\n",
    "    axes[1].set_ylabel(\"fit_times\")\n",
    "    axes[1].set_title(\"Scalability of the model\")\n",
    "\n",
    "    # Plot fit_time vs score\n",
    "    axes[2].grid()\n",
    "    axes[2].plot(fit_times_mean, test_scores_mean, 'o-')\n",
    "    axes[2].fill_between(fit_times_mean, test_scores_mean - test_scores_std,\n",
    "                         test_scores_mean + test_scores_std, alpha=0.1)\n",
    "    axes[2].set_xlabel(\"fit_times\")\n",
    "    axes[2].set_ylabel(\"Score\")\n",
    "    axes[2].set_title(\"Performance of the model\")\n",
    "\n",
    "    return plt\n",
    "\n",
    "# Feature correlation heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(features_filled.corr(), annot=True, cmap='coolwarm')\n",
    "plt.title('Feature Correlation Heatmap')\n",
    "plt.show()\n",
    "\n",
    "# Dictionary to store model results\n",
    "model_results = {}\n",
    "\n",
    "# Function to evaluate and store model results\n",
    "def evaluate_model(model, X_test, y_test, model_name):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    model_results[model_name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'roc_auc': roc_auc,\n",
    "        'fpr': fpr,\n",
    "        'tpr': tpr\n",
    "    }\n",
    "    \n",
    "    print(f\"{model_name} Accuracy Score: {accuracy}\")\n",
    "    print(f\"{model_name} Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    ConfusionMatrixDisplay(conf_matrix, display_labels=['Nein', 'Ja']).plot(cmap='viridis')\n",
    "    plt.title(f'Confusion Matrix for {model_name}')\n",
    "    plt.show()\n",
    "    \n",
    "    # Learning Curve\n",
    "    plot_learning_curve(model, f'Learning Curve for {model_name}', X_train_scaled, y_train)\n",
    "    plt.show()\n",
    "\n",
    "# Logistic Regression\n",
    "param_grid_lr = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear', 'saga']\n",
    "}\n",
    "log_reg = LogisticRegression()\n",
    "grid_search_lr = GridSearchCV(log_reg, param_grid_lr, cv=5, scoring='accuracy')\n",
    "grid_search_lr.fit(X_train_scaled, y_train)\n",
    "best_lr_model = grid_search_lr.best_estimator_\n",
    "print(\"Best Parameters for Logistic Regression:\", grid_search_lr.best_params_)\n",
    "evaluate_model(best_lr_model, X_test_scaled, y_test, \"Logistic Regression\")\n",
    "\n",
    "# Random Forest\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "grid_search_rf = GridSearchCV(rf, param_grid_rf, cv=5, scoring='accuracy')\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "best_rf_model = grid_search_rf.best_estimator_\n",
    "print(\"Best Parameters for Random Forest:\", grid_search_rf.best_params_)\n",
    "evaluate_model(best_rf_model, X_test, y_test, \"Random Forest\")\n",
    "\n",
    "# Feature importance for Random Forest\n",
    "feature_importance_rf = pd.DataFrame({\n",
    "    'Feature': features_filled.columns,\n",
    "    'Importance': best_rf_model.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance_rf, palette='viridis')\n",
    "plt.title('Random Forest Feature Importance')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Gradient Boosting\n",
    "param_grid_gb = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "grid_search_gb = GridSearchCV(gb, param_grid_gb, cv=5, scoring='accuracy')\n",
    "grid_search_gb.fit(X_train_scaled, y_train)\n",
    "best_gb_model = grid_search_gb.best_estimator_\n",
    "print(\"Best Parameters for Gradient Boosting:\", grid_search_gb.best_params_)\n",
    "evaluate_model(best_gb_model, X_test_scaled, y_test, \"Gradient Boosting\")\n",
    "\n",
    "# K-Nearest Neighbors\n",
    "param_grid_knn = {\n",
    "    'n_neighbors': [3, 5, 7],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'p': [1, 2]\n",
    "}\n",
    "knn = KNeighborsClassifier()\n",
    "grid_search_knn = GridSearchCV(knn, param_grid_knn, cv=5, scoring='accuracy')\n",
    "grid_search_knn.fit(X_train_scaled, y_train)\n",
    "best_knn_model = grid_search_knn.best_estimator_\n",
    "print(\"Best Parameters for KNN:\", grid_search_knn.best_params_)\n",
    "evaluate_model(best_knn_model, X_test_scaled, y_test, \"KNN\")\n",
    "\n",
    "# Support Vector Machine\n",
    "param_grid_svm = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "svm = SVC(probability=True, random_state=42)\n",
    "grid_search_svm = GridSearchCV(svm, param_grid_svm, cv=5, scoring='accuracy')\n",
    "grid_search_svm.fit(X_train_scaled, y_train)\n",
    "best_svm_model = grid_search_svm.best_estimator_\n",
    "print(\"Best Parameters for SVM:\", grid_search_svm.best_params_)\n",
    "evaluate_model(best_svm_model, X_test_scaled, y_test, \"SVM\")\n",
    "\n",
    "# LightGBM\n",
    "param_grid_lgbm = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'num_leaves': [7, 15, 31]  # Adjusted values\n",
    "}\n",
    "lgbm = LGBMClassifier(random_state=42, min_child_samples=1)\n",
    "grid_search_lgbm = GridSearchCV(lgbm, param_grid_lgbm, cv=5, scoring='accuracy')\n",
    "grid_search_lgbm.fit(X_train, y_train)\n",
    "best_lgbm_model = grid_search_lgbm.best_estimator_\n",
    "print(\"Best Parameters for LightGBM:\", grid_search_lgbm.best_params_)\n",
    "evaluate_model(best_lgbm_model, X_test, y_test, \"LightGBM\")\n",
    "\n",
    "# Voting Classifier\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('lr', best_lr_model),\n",
    "        ('rf', best_rf_model),\n",
    "        ('gb', best_gb_model),\n",
    "        ('svm', best_svm_model),\n",
    "        ('lgbm', best_lgbm_model)\n",
    "    ],\n",
    "    voting='soft'\n",
    ")\n",
    "voting_clf.fit(X_train_scaled, y_train)\n",
    "evaluate_model(voting_clf, X_test_scaled, y_test, \"Voting Classifier\")\n",
    "\n",
    "# Model Comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "metrics = ['accuracy', 'precision', 'recall', 'roc_auc']\n",
    "for metric in metrics:\n",
    "    values = [results[metric] for results in model_results.values()]\n",
    "    plt.bar(model_results.keys(), values, label=metric)\n",
    "\n",
    "plt.title('Model Comparison')\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Score')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve for all models\n",
    "plt.figure(figsize=(10, 8))\n",
    "for model_name, results in model_results.items():\n",
    "    plt.plot(results['fpr'], results['tpr'], label=f'{model_name} (AUC = {results[\"roc_auc\"]:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Hyperparameter tuning visualization for Random Forest\n",
    "rf_results = grid_search_rf.cv_results_\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(131)\n",
    "plt.scatter(rf_results['param_n_estimators'], rf_results['mean_test_score'])\n",
    "plt.xlabel('n_estimators')\n",
    "plt.ylabel('Mean test score')\n",
    "plt.title('Effect of n_estimators on RF performance')\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.scatter(rf_results['param_max_depth'], rf_results['mean_test_score'])\n",
    "plt.xlabel('max_depth')\n",
    "plt.ylabel('Mean test score')\n",
    "plt.title('Effect of max_depth on RF performance')\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.scatter(rf_results['param_min_samples_split'], rf_results['mean_test_score'])\n",
    "plt.xlabel('min_samples_split')\n",
    "plt.ylabel('Mean test score')\n",
    "plt.title('Effect of min_samples_split on RF performance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py12-PO",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
